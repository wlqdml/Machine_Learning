{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e9e6e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import collections\n",
    "from collections import Counter\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50d09f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv('surnames_with_splits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52785cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the data:  (10980, 4)\n",
      "------------------------------------------------------------\n",
      "  nationality  nationality_index  split   surname\n",
      "0      Arabic                 15  train     Totah\n",
      "1      Arabic                 15  train    Abboud\n",
      "2      Arabic                 15  train  Fakhoury\n",
      "3      Arabic                 15  train     Srour\n",
      "4      Arabic                 15  train    Sayegh\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of the data: \", df_all.shape)\n",
    "print('-'*60)\n",
    "print(df_all.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afde8402",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Class to process text and extract vocabulary for mapping\"\"\"\n",
    "    def __init__(self, token_to_idx=None, add_unk=True, unk_token=\"<UNK>\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx (dict): a pre-existing map of tokens to indices\n",
    "            add_unk (bool): a flag that indicates whether to add the UNK token\n",
    "            unk_token (str): the UNK token to add into the Vocabulary\n",
    "        \"\"\"\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "        self._idx_to_token = {idx: token \n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "        self._add_unk   = add_unk\n",
    "        self._unk_token = unk_token      \n",
    "        self.unk_index  = -999\n",
    "        ### the unk_token, i.e, \"<UNK>\" is the first added token if add_unk=True\n",
    "        ### self.unk_index is changed from -999 to 0\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token) \n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\"Update mapping dicts based on the token.\n",
    "\n",
    "        Args:\n",
    "            token (str): the item to add into the Vocabulary\n",
    "        Returns:\n",
    "            index (int): the integer corresponding to the token\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "   \n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "          or the UNK index if token isn't present.\n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary) \n",
    "              for the UNK functionality \n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            ### .get(): return self.unk_index if the key \"token\" does not exist. \n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "    \n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"Return the token associated with the index\n",
    "        \n",
    "        Args: \n",
    "            index (int): the index to look up\n",
    "        Returns:\n",
    "            token (str): the token corresponding to the index\n",
    "        Raises:\n",
    "            KeyError: if the index is not in the Vocabulary\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbdb2b7",
   "metadata": {},
   "source": [
    "# 1. SurnameVectorizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e2aadac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameVectorizer(object):\n",
    "    \"\"\" The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"\n",
    "    def __init__(self, surname_vocab, nationality_vocab):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            surname_vocab (Vocabulary): maps characters to integers\n",
    "            nationality_vocab (Vocabulary): maps nationalities to integers\n",
    "        \"\"\"\n",
    "        self.surname_vocab = surname_vocab\n",
    "        self.nationality_vocab = nationality_vocab\n",
    "         \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, surname_df):\n",
    "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
    "        \n",
    "        Args:\n",
    "            surname_df (pandas.DataFrame): the surnames dataset\n",
    "        Returns:\n",
    "            an instance of the SurnameVectorizer\n",
    "        \"\"\"\n",
    "        surname_vocab     = Vocabulary(add_unk=True, unk_token=\"@\")\n",
    "        nationality_vocab = Vocabulary(add_unk=False)\n",
    "        \n",
    "        ########## Add tokens to surname_vocab and nationality_vocab\n",
    "        for index, row in surname_df.iterrows():\n",
    "            # Add tokens(characters) to surname_vocab\n",
    "            for letter in row.surname:\n",
    "                surname_vocab.add_token(letter)\n",
    "            # Add tokens(words) to nationality_vocab\n",
    "            nationality_vocab.add_token(row.nationality)\n",
    "\n",
    "        return cls(surname_vocab, nationality_vocab)\n",
    "\n",
    "    ### This is the key functionality of the Vectorizer.\n",
    "    ### It takes as an argument a string representing a surname,\n",
    "    ### and returns a vectorized representation of the surname.\n",
    "    def vectorize(self, surname):\n",
    "        \"\"\"\n",
    "        Create a collapsed one-hot representation vector for the surname\n",
    "        Limitations of the one-hot method:\n",
    "        1 - Sparseness, n_unique_words in a surname << n_unique_words in a vocabulary\n",
    "        2 - Discarding the order of the words' appearance\n",
    "        \n",
    "        Args:\n",
    "            surname (str): the surname \n",
    "        Returns:\n",
    "            one_hot (np.ndarray): the collapsed one-hot encoding \n",
    "        \"\"\"\n",
    "        ### Create an array where each element corresponds to each character in the vocabulary\n",
    "        one_hot = np.zeros(len(self.surname_vocab), dtype=np.float32)\n",
    "        ### Run lookup_token() for each character in the surname sequentially, return an index\n",
    "        ### Assign the corresponding element in the array to 1.\n",
    "        for token in surname:\n",
    "            one_hot[self.surname_vocab.lookup_token(token)] = 1\n",
    "        return one_hot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f268a8b6",
   "metadata": {},
   "source": [
    "# 2. Instantiate the SurnameVectorizer from the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1558ed85",
   "metadata": {},
   "source": [
    "### First draw a (static, fixed random seed) from the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "667e8d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_all.sample(100,random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81a4d269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nationality</th>\n",
       "      <th>nationality_index</th>\n",
       "      <th>split</th>\n",
       "      <th>surname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5362</th>\n",
       "      <td>English</td>\n",
       "      <td>12</td>\n",
       "      <td>test</td>\n",
       "      <td>Hepples</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>15</td>\n",
       "      <td>train</td>\n",
       "      <td>Sarkis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10686</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>6</td>\n",
       "      <td>train</td>\n",
       "      <td>Garza</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8902</th>\n",
       "      <td>Russian</td>\n",
       "      <td>13</td>\n",
       "      <td>train</td>\n",
       "      <td>Zhurikhin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>15</td>\n",
       "      <td>val</td>\n",
       "      <td>Aswad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      nationality  nationality_index  split    surname\n",
       "5362      English                 12   test    Hepples\n",
       "900        Arabic                 15  train     Sarkis\n",
       "10686     Spanish                  6  train      Garza\n",
       "8902      Russian                 13  train  Zhurikhin\n",
       "1296       Arabic                 15    val      Aswad"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2041ed49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>split</th>\n",
       "      <th>test</th>\n",
       "      <th>train</th>\n",
       "      <th>val</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nationality</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Arabic</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chinese</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Czech</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>English</th>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>French</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>German</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Irish</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Italian</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Japanese</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Korean</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Polish</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Russian</th>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spanish</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "split        test  train  val\n",
       "nationality                  \n",
       "Arabic          3     12    2\n",
       "Chinese         1      2    0\n",
       "Czech           1      2    0\n",
       "English         4     18    1\n",
       "French          0      2    0\n",
       "German          1      4    1\n",
       "Irish           0      3    0\n",
       "Italian         2      5    1\n",
       "Japanese        2      4    3\n",
       "Korean          0      1    0\n",
       "Polish          0      2    0\n",
       "Russian         5     12    2\n",
       "Spanish         1      3    0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(df_sample['nationality'], df_sample['split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d526fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = SurnameVectorizer.from_dataframe(df_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378100ce",
   "metadata": {},
   "source": [
    "### A vectorizer has two vocabularies(attributes), one for surname, one for nationality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3621bd23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'surname_vocab': <__main__.Vocabulary at 0x7f7fe4a77970>,\n",
       " 'nationality_vocab': <__main__.Vocabulary at 0x7f7fe4a77f10>}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad80b5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nationality_vocab\n",
      "{'English': 0, 'Arabic': 1, 'Spanish': 2, 'Russian': 3, 'Japanese': 4, 'Chinese': 5, 'German': 6, 'Italian': 7, 'Irish': 8, 'Polish': 9, 'Korean': 10, 'French': 11, 'Czech': 12}\n",
      "{0: 'English', 1: 'Arabic', 2: 'Spanish', 3: 'Russian', 4: 'Japanese', 5: 'Chinese', 6: 'German', 7: 'Italian', 8: 'Irish', 9: 'Polish', 10: 'Korean', 11: 'French', 12: 'Czech'}\n",
      "------------------------------------------------------------\n",
      "surname_vocab\n",
      "Includes 49 tokens\n"
     ]
    }
   ],
   "source": [
    "print('nationality_vocab')\n",
    "print(vectorizer.nationality_vocab._token_to_idx)\n",
    "print(vectorizer.nationality_vocab._idx_to_token)\n",
    "print('-'*60)\n",
    "print('surname_vocab')\n",
    "print(f\"Includes {len(vectorizer.surname_vocab)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b2486c",
   "metadata": {},
   "source": [
    "# 3. Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225f78a7",
   "metadata": {},
   "source": [
    "### (classmethod) from_dataframe(surname_df): Instantiate the vectorizer from the dataset dataframe.\n",
    "1. First instantiate two Vocabularies based on the input data \"surname_df\". [See a walkthrough of Vocabulary class here](https://github.com/houzhj/Machine_Learning/blob/main/ipynb/Surname_Nationality/class_Vocabulary.ipynb).\n",
    "2. Use the surname_vocab and nationality_vocab as inputs to instantiate a vectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144b5097",
   "metadata": {},
   "source": [
    "### vectorize(surname): It takes as an argument a string representing a surname, and returns a vectorized representation of the surname. This is the key functionality of the Vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3aecb11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_surname = \"Onizuka\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45e3d27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surname_vocab: {0: '@', 1: 'H', 2: 'e', 3: 'p', 4: 'l', 5: 's', 6: 'S', 7: 'a', 8: 'r', 9: 'k', 10: 'i', 11: 'G', 12: 'z', 13: 'Z', 14: 'h', 15: 'u', 16: 'n', 17: 'A', 18: 'w', 19: 'd', 20: 't', 21: 'c', 22: 'L', 23: 'b', 24: 'o', 25: 'Y', 26: 'v', 27: 'g', 28: 'y', 29: 'm', 30: 'E', 31: 'V', 32: 'D', 33: 'N', 34: 'T', 35: 'C', 36: 'J', 37: 'K', 38: 'M', 39: 'W', 40: 'j', 41: 'B', 42: 'P', 43: 'q', 44: 'F', 45: 'Ã¨', 46: 'O', 47: 'f', 48: 'R'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "One-hot representation: [0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0.]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = SurnameVectorizer.from_dataframe(df_sample)\n",
    "one_hot    = vectorizer.vectorize(example_surname)\n",
    "print('Surname_vocab:',vectorizer.surname_vocab._idx_to_token)\n",
    "print('-'*100)\n",
    "print('One-hot representation:', one_hot)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
